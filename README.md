# familiar-ai-gui

A desktop app that gives an AI a body â€” camera (eyes & neck), voice, robot legs, and episodic memory.
Built with Tauri + React + Rust.

> ğŸ“– [æ—¥æœ¬èª](README-ja.md) | [ä¸­æ–‡](README-zh.md) | [ç¹é«”ä¸­æ–‡](README-zh-TW.md) | [FranÃ§ais](README-fr.md) | [Deutsch](README-de.md)

## Features

- **Multi-LLM** â€” Kimi (Moonshot) / Claude (Anthropic) / Gemini (Google) / GPT (OpenAI)
- **Eyes & neck** â€” ONVIF PTZ camera for vision and pan/tilt (`see` / `look`)
- **Voice** â€” ElevenLabs TTS with real-time speech (`say`)
- **Legs** â€” Tuya robot vacuum for locomotion (`walk`)
- **Memory** â€” Episodic memory via SQLite + 384-dim embedding vectors (`remember` / `recall`)
- **Desire system** â€” Intrinsic motivation: the AI acts spontaneously when desires grow strong

Camera, TTS, and mobility are all optional â€” only an LLM API key is required to run.

---

## Prerequisites

| Tool | Version | Install |
|------|---------|---------|
| Node.js | 18+ | [nodejs.org](https://nodejs.org/) |
| Rust | 1.80+ | `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs \| sh` |
| Tauri CLI v2 | 2.x | `cargo install tauri-cli --version "^2"` |
| ffmpeg | any | Required only for RTSP camera snapshots |

---

## Setup & Run

```bash
# 1. Clone
git clone https://github.com/lifemate-ai/familiar-ai-gui.git
cd familiar-ai-gui

# 2. Install frontend dependencies
npm install

# 3. Start in development mode (hot reload)
npm run tauri dev
```

A setup wizard opens on first launch â€” enter your LLM API key and persona (3 steps).

### Production build

```bash
npm run tauri build
# Output: src-tauri/target/release/bundle/
#   Linux:   .AppImage / .deb
#   macOS:   .dmg
#   Windows: .msi / .exe
```

---

## Configuration

Settings are saved by the wizard, but can also be edited directly.

**Location:**
- Linux/macOS: `~/.config/familiar-ai/config.toml`
- Windows: `%APPDATA%\familiar-ai\config.toml`

```toml
platform = "kimi"          # kimi | anthropic | gemini | openai
api_key = "sk-..."         # LLM API key (required)
model = ""                 # Leave empty for platform default (see table below)
agent_name = "Kokone"      # AI's name
persona = "..."            # Persona description injected into system prompt
companion_name = "Kouta"   # Your name

# ONVIF PTZ camera (optional)
[camera]
host = "192.168.1.100"
username = "admin"
password = "password"
onvif_port = 2020

# ElevenLabs TTS (optional)
[tts]
elevenlabs_api_key = "sk_..."
voice_id = "cgSgspJ2msm6clMCkdW9"

# Tuya robot vacuum (optional)
[mobility]
tuya_region = "us"         # us | eu | in
tuya_api_key = "..."
tuya_api_secret = "..."
tuya_device_id = "..."
```

### Default models by platform

| platform | default model |
|----------|--------------|
| `kimi` | `kimi-k2.5` |
| `anthropic` | `claude-haiku-4-5-20251001` |
| `gemini` | `gemini-2.5-flash` |
| `openai` | `gpt-4o-mini` |

---

## Tools

The agent can use the following tools:

| Tool | Args | Description |
|------|------|-------------|
| `see` | â€” | Capture a camera snapshot and show it to the AI |
| `look` | `direction` (left/right/up/down/around), `degrees` (1â€“90) | Pan/tilt the camera |
| `say` | `text`, `speaker` (camera/pc/both) | Speak aloud via ElevenLabs TTS |
| `walk` | `direction` (forward/backward/left/right/stop), `duration` (s, optional) | Move the robot vacuum |
| `remember` | `content`, `emotion`, `image_path` (optional) | Save an episodic memory |
| `recall` | `query`, `n` (count) | Semantic memory search |

---

## Data

| Data | Path |
|------|------|
| Config | `~/.config/familiar-ai/config.toml` |
| Memory database | `~/.familiar_ai/observations.db` (SQLite) |

---

## Testing

```bash
cd src-tauri
cargo test --lib
# â†’ 201 tests passing
```

---

## Architecture

```
React frontend (Vite)
    â†• Tauri IPC (invoke / event)
Rust backend
    â”œâ”€â”€ agent.rs        â€” ReAct agent loop + desire-driven idle ticks
    â”œâ”€â”€ desires.rs      â€” Desire system (observe_room / look_outside /
    â”‚                     browse_curiosity / miss_companion)
    â”œâ”€â”€ backend/        â€” Multi-LLM adapters
    â”‚   â”œâ”€â”€ kimi.rs
    â”‚   â”œâ”€â”€ anthropic.rs
    â”‚   â”œâ”€â”€ gemini.rs
    â”‚   â””â”€â”€ openai.rs
    â””â”€â”€ tools/
        â”œâ”€â”€ camera.rs   â€” ONVIF PTZ + RTSP snapshot
        â”œâ”€â”€ tts.rs      â€” ElevenLabs TTS + Tapo camera speaker
        â”œâ”€â”€ tapo_audio.rs â€” Tapo HTTP Stream audio backchannel
        â”œâ”€â”€ mobility.rs â€” Tuya API (HMAC-SHA256 signing)
        â””â”€â”€ memory.rs   â€” SQLite + fastembed embedding vectors
```

The agent runs a ReAct loop: build world model â†’ recall memories â†’ LLM streaming â†’ execute tools â†’ feedback â†’ repeat.

A heartbeat thread fires an idle tick every 60 seconds when a desire exceeds the action threshold, enabling spontaneous behaviour without user input.

---

## IDE Setup

[VS Code](https://code.visualstudio.com/) + [Tauri](https://marketplace.visualstudio.com/items?itemName=tauri-apps.tauri-vscode) + [rust-analyzer](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer)
